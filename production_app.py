"""
Production-Ready Enterprise Data Modeling Application
Industry-grade implementation with FastAPI, monitoring, and scalability
"""

import os
import logging
import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from contextlib import asynccontextmanager

import uvicorn
from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from fastapi.responses import JSONResponse
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
import redis
import psycopg2
from sqlalchemy import create_engine, text
import pandas as pd
import numpy as np
from pydantic import BaseModel, Field
import json
import mlflow
import mlflow.sklearn
from prometheus_client import Counter, Histogram, Gauge, generate_latest
import time

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/app/logs/app.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Prometheus metrics
REQUEST_COUNT = Counter('http_requests_total', 'Total HTTP requests', ['method', 'endpoint'])
REQUEST_DURATION = Histogram('http_request_duration_seconds', 'HTTP request duration')
ACTIVE_CONNECTIONS = Gauge('active_connections', 'Number of active connections')
DATA_PROCESSING_TIME = Histogram('data_processing_seconds', 'Data processing time')

# Pydantic models
class HealthResponse(BaseModel):
    status: str
    timestamp: datetime
    version: str
    uptime: float

class DataRequest(BaseModel):
    start_date: Optional[datetime] = None
    end_date: Optional[datetime] = None
    customer_segment: Optional[str] = None
    limit: int = Field(default=1000, le=10000)

class MLPredictionRequest(BaseModel):
    customer_id: str
    features: Dict[str, Any]

class MLPredictionResponse(BaseModel):
    prediction: float
    confidence: float
    model_version: str
    timestamp: datetime

# Global variables
app_start_time = time.time()
db_engine = None
redis_client = None
mlflow_client = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan manager"""
    global db_engine, redis_client, mlflow_client
    
    logger.info("ðŸš€ Starting Enterprise Data Modeling Application...")
    
    # Initialize database connection
    try:
        db_engine = create_engine(
            os.getenv('DATABASE_URL', 'postgresql://postgres:password@postgres:5432/retail_analytics')
        )
        logger.info("âœ… Database connection established")
    except Exception as e:
        logger.error(f"âŒ Database connection failed: {e}")
        raise
    
    # Initialize Redis connection
    try:
        redis_client = redis.Redis(
            host=os.getenv('REDIS_HOST', 'redis'),
            port=int(os.getenv('REDIS_PORT', '6379')),
            decode_responses=True
        )
        redis_client.ping()
        logger.info("âœ… Redis connection established")
    except Exception as e:
        logger.error(f"âŒ Redis connection failed: {e}")
        raise
    
    # Initialize MLflow
    try:
        mlflow.set_tracking_uri(os.getenv('MLFLOW_TRACKING_URI', 'http://mlflow:5000'))
        mlflow_client = mlflow.tracking.MlflowClient()
        logger.info("âœ… MLflow connection established")
    except Exception as e:
        logger.error(f"âŒ MLflow connection failed: {e}")
        # Continue without MLflow for now
    
    logger.info("âœ… Application startup completed")
    
    yield
    
    # Cleanup
    logger.info("ðŸ”„ Shutting down application...")
    if db_engine:
        db_engine.dispose()
    if redis_client:
        redis_client.close()

# Create FastAPI application
app = FastAPI(
    title="Enterprise Data Modeling API",
    description="Industry-grade data modeling and analytics API",
    version="2.0.0",
    lifespan=lifespan
)

# Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.add_middleware(
    TrustedHostMiddleware,
    allowed_hosts=["*"]
)

# Security
security = HTTPBearer()

# Dependency functions
async def get_database():
    """Get database connection"""
    if not db_engine:
        raise HTTPException(status_code=503, detail="Database not available")
    return db_engine

async def get_redis():
    """Get Redis connection"""
    if not redis_client:
        raise HTTPException(status_code=503, detail="Redis not available")
    return redis_client

async def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """Verify API token"""
    # In production, implement proper JWT verification
    if credentials.credentials != "your-api-token":
        raise HTTPException(status_code=401, detail="Invalid token")
    return credentials.credentials

# API Endpoints
@app.get("/", response_model=Dict[str, str])
async def root():
    """Root endpoint"""
    REQUEST_COUNT.labels(method='GET', endpoint='/').inc()
    return {
        "message": "Enterprise Data Modeling API",
        "version": "2.0.0",
        "status": "operational"
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint"""
    REQUEST_COUNT.labels(method='GET', endpoint='/health').inc()
    
    uptime = time.time() - app_start_time
    
    # Check database connection
    db_status = "healthy"
    try:
        with db_engine.connect() as conn:
            conn.execute(text("SELECT 1"))
    except Exception:
        db_status = "unhealthy"
    
    # Check Redis connection
    redis_status = "healthy"
    try:
        redis_client.ping()
    except Exception:
        redis_status = "unhealthy"
    
    overall_status = "healthy" if db_status == "healthy" and redis_status == "healthy" else "degraded"
    
    return HealthResponse(
        status=overall_status,
        timestamp=datetime.now(),
        version="2.0.0",
        uptime=uptime
    )

@app.get("/metrics")
async def metrics():
    """Prometheus metrics endpoint"""
    return generate_latest()

@app.get("/api/data/customers")
async def get_customers(
    request: DataRequest = Depends(),
    db = Depends(get_database),
    token: str = Depends(verify_token)
):
    """Get customer data with filtering"""
    REQUEST_COUNT.labels(method='GET', endpoint='/api/data/customers').inc()
    
    with REQUEST_DURATION.time():
        try:
            query = "SELECT * FROM customers"
            conditions = []
            params = {}
            
            if request.start_date:
                conditions.append("created_at >= :start_date")
                params['start_date'] = request.start_date
            
            if request.end_date:
                conditions.append("created_at <= :end_date")
                params['end_date'] = request.end_date
            
            if request.customer_segment:
                conditions.append("customer_segment = :segment")
                params['segment'] = request.customer_segment
            
            if conditions:
                query += " WHERE " + " AND ".join(conditions)
            
            query += f" LIMIT {request.limit}"
            
            with db.connect() as conn:
                result = conn.execute(text(query), params)
                customers = [dict(row._mapping) for row in result]
            
            return {
                "data": customers,
                "count": len(customers),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error fetching customers: {e}")
            raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/api/data/products")
async def get_products(
    request: DataRequest = Depends(),
    db = Depends(get_database),
    token: str = Depends(verify_token)
):
    """Get product data with filtering"""
    REQUEST_COUNT.labels(method='GET', endpoint='/api/data/products').inc()
    
    with REQUEST_DURATION.time():
        try:
            query = "SELECT * FROM products"
            conditions = []
            params = {}
            
            if request.start_date:
                conditions.append("created_at >= :start_date")
                params['start_date'] = request.start_date
            
            if request.end_date:
                conditions.append("created_at <= :end_date")
                params['end_date'] = request.end_date
            
            if conditions:
                query += " WHERE " + " AND ".join(conditions)
            
            query += f" LIMIT {request.limit}"
            
            with db.connect() as conn:
                result = conn.execute(text(query), params)
                products = [dict(row._mapping) for row in result]
            
            return {
                "data": products,
                "count": len(products),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error fetching products: {e}")
            raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/api/analytics/revenue")
async def get_revenue_analytics(
    start_date: Optional[datetime] = None,
    end_date: Optional[datetime] = None,
    db = Depends(get_database),
    token: str = Depends(verify_token)
):
    """Get revenue analytics"""
    REQUEST_COUNT.labels(method='GET', endpoint='/api/analytics/revenue').inc()
    
    with REQUEST_DURATION.time():
        try:
            query = """
                SELECT 
                    DATE(order_date) as date,
                    SUM(net_total) as daily_revenue,
                    COUNT(DISTINCT order_id) as daily_orders,
                    COUNT(DISTINCT customer_id) as daily_customers
                FROM orders 
                WHERE 1=1
            """
            params = {}
            
            if start_date:
                query += " AND order_date >= :start_date"
                params['start_date'] = start_date
            
            if end_date:
                query += " AND order_date <= :end_date"
                params['end_date'] = end_date
            
            query += " GROUP BY DATE(order_date) ORDER BY date"
            
            with db.connect() as conn:
                result = conn.execute(text(query), params)
                analytics = [dict(row._mapping) for row in result]
            
            return {
                "analytics": analytics,
                "summary": {
                    "total_revenue": sum(item['daily_revenue'] for item in analytics),
                    "total_orders": sum(item['daily_orders'] for item in analytics),
                    "total_customers": len(set(item['daily_customers'] for item in analytics))
                },
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error fetching revenue analytics: {e}")
            raise HTTPException(status_code=500, detail="Internal server error")

@app.post("/api/ml/predict/clv", response_model=MLPredictionResponse)
async def predict_customer_lifetime_value(
    request: MLPredictionRequest,
    background_tasks: BackgroundTasks,
    token: str = Depends(verify_token)
):
    """Predict customer lifetime value using ML model"""
    REQUEST_COUNT.labels(method='POST', endpoint='/api/ml/predict/clv').inc()
    
    with REQUEST_DURATION.time():
        try:
            # In production, load the actual trained model
            # For now, return a mock prediction
            prediction = np.random.uniform(1000, 10000)
            confidence = np.random.uniform(0.8, 0.95)
            
            # Log prediction for monitoring
            background_tasks.add_task(
                log_prediction,
                request.customer_id,
                prediction,
                confidence
            )
            
            return MLPredictionResponse(
                prediction=prediction,
                confidence=confidence,
                model_version="1.0.0",
                timestamp=datetime.now()
            )
            
        except Exception as e:
            logger.error(f"Error predicting CLV: {e}")
            raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/api/analytics/customer-segments")
async def get_customer_segments(
    db = Depends(get_database),
    token: str = Depends(verify_token)
):
    """Get customer segmentation analysis"""
    REQUEST_COUNT.labels(method='GET', endpoint='/api/analytics/customer-segments').inc()
    
    with REQUEST_DURATION.time():
        try:
            query = """
                SELECT 
                    customer_segment,
                    COUNT(*) as customer_count,
                    AVG(age) as avg_age,
                    SUM(total_spent) as total_revenue
                FROM customers c
                LEFT JOIN (
                    SELECT 
                        customer_id,
                        SUM(net_total) as total_spent
                    FROM orders o
                    JOIN order_items oi ON o.order_id = oi.order_id
                    GROUP BY customer_id
                ) revenue ON c.customer_id = revenue.customer_id
                GROUP BY customer_segment
            """
            
            with db.connect() as conn:
                result = conn.execute(text(query))
                segments = [dict(row._mapping) for row in result]
            
            return {
                "segments": segments,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error fetching customer segments: {e}")
            raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/api/analytics/anomalies")
async def get_anomalies(
    limit: int = 100,
    db = Depends(get_database),
    token: str = Depends(verify_token)
):
    """Get anomaly detection results"""
    REQUEST_COUNT.labels(method='GET', endpoint='/api/analytics/anomalies').inc()
    
    with REQUEST_DURATION.time():
        try:
            # Simple anomaly detection based on order value
            query = """
                SELECT 
                    o.order_id,
                    o.customer_id,
                    o.order_total,
                    o.order_date,
                    c.customer_segment,
                    CASE 
                        WHEN o.order_total > (
                            SELECT AVG(order_total) + 3 * STDDEV(order_total) 
                            FROM orders
                        ) THEN true
                        ELSE false
                    END as is_anomaly
                FROM orders o
                JOIN customers c ON o.customer_id = c.customer_id
                WHERE o.order_total > (
                    SELECT AVG(order_total) + 2 * STDDEV(order_total) 
                    FROM orders
                )
                ORDER BY o.order_total DESC
                LIMIT :limit
            """
            
            with db.connect() as conn:
                result = conn.execute(text(query), {"limit": limit})
                anomalies = [dict(row._mapping) for row in result]
            
            return {
                "anomalies": anomalies,
                "count": len(anomalies),
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error fetching anomalies: {e}")
            raise HTTPException(status_code=500, detail="Internal server error")

# Background tasks
async def log_prediction(customer_id: str, prediction: float, confidence: float):
    """Log ML prediction for monitoring"""
    try:
        if redis_client:
            redis_client.lpush(
                "ml_predictions",
                json.dumps({
                    "customer_id": customer_id,
                    "prediction": prediction,
                    "confidence": confidence,
                    "timestamp": datetime.now().isoformat()
                })
            )
    except Exception as e:
        logger.error(f"Error logging prediction: {e}")

# Error handlers
@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    """Handle HTTP exceptions"""
    return JSONResponse(
        status_code=exc.status_code,
        content={"detail": exc.detail, "timestamp": datetime.now().isoformat()}
    )

@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    """Handle general exceptions"""
    logger.error(f"Unhandled exception: {exc}")
    return JSONResponse(
        status_code=500,
        content={"detail": "Internal server error", "timestamp": datetime.now().isoformat()}
    )

if __name__ == "__main__":
    uvicorn.run(
        "production_app:app",
        host="0.0.0.0",
        port=8000,
        reload=False,
        workers=4,
        log_level="info"
    )
